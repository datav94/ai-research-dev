{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403f8e2f-db56-4093-9949-7ee6a6d34c9d",
   "metadata": {},
   "source": [
    "# __Convolution Neural Network__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9508a2c7-913b-48b7-9af7-3248ea4cefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93b2e7f8-4c85-4658-a581-b94247c756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a87c8c2c-3629-44d1-8ef4-a08cc55118f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\sysadmin\\\\ai-research-dev\\\\Machine Learning\\\\Cats and dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0efe0a78-94cd-4c55-98e3-6fe2661fddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e28b7d3-75c2-4d56-bda2-ca0cd35a669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Activation, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9c57ff1-06ec-4eaf-8e6d-dd2b5a820dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = base_dir + '\\\\train'\n",
    "validation_dir = base_dir + '\\\\validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7cfd8-87de-42ea-b46b-1101571d2ccc",
   "metadata": {},
   "source": [
    "#### All images need to the of same size to go through CNN\n",
    "\n",
    "#### we are gonna stick with 255 x 255 x 3 size. where height = 255 px, breadth = 255 px and color channel = 3 (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd4c412b-3168-42bb-a015-6d58d9973a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3,3), input_shape=(250,250,3)))  # Conv2d(filters, (size x size of filter))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))  # this is second layer and so this and further layers do not require an input shape \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fe9f0d5-9989-47a8-a91f-0a033d551365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 248, 248, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 248, 248, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 122, 122, 32)      18464     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 122, 122, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 59, 59, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 59, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "=================================================================\n",
      "Total params: 38,752\n",
      "Trainable params: 38,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0551022-b595-4c32-bc76-e670a6952f33",
   "metadata": {},
   "source": [
    "###  Now we need a flattening layer and an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "348f92b3-8a57-46bc-b5d2-9c30949e0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bd062-286e-445a-96e3-6fb3147ff0aa",
   "metadata": {},
   "source": [
    "### Now we are gonna build a ANN around our flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85376c1f-4862-49f5-876c-3f9e914f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2992cf54-c43b-41f9-8ab5-b8e135ba054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f17bf4f6-c993-4552-bae4-cd4a37bd3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 248, 248, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 248, 248, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 122, 122, 32)      18464     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 122, 122, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 59, 59, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 59, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 741,473\n",
      "Trainable params: 741,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf896f98-bca6-4317-bf7b-4d0d5ecb8a7e",
   "metadata": {},
   "source": [
    "## Once we are done with creating the model we need to compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f894143-d843-4700-be0f-f5985d79278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2907707-9638-4563-9d21-ad9574340a5c",
   "metadata": {},
   "source": [
    "# Now we need to get the data and then train the model\n",
    "\n",
    "- We are gonna need image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15206950-d85e-4050-af45-bd29f2b75beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b47952e5-ec05-47af-8e3d-f58cd74cd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the generator that we shall use to read our images as images in the training set or in any dataset\n",
    "# can be of various sizes and features and we need to convert them to the required size of 250x250x3 \n",
    "# ImageDataGenerator does that job for us\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, # the dot is just to convert to float\n",
    "    rotation_range=15, # to rotate any image randomly upto 15 degrees\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# test data cannot have any other manipulations otherwise the results wont be reliable\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7df138b8-1aba-40b3-82d9-e4699bf3edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74546c23-9cb3-4fa0-b33e-3814b46b84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1602 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(250,250),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(250,250),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f6ab7-4d79-4589-bbdb-5c37e5925751",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11994b79-91fc-47f5-90bd-8d9ecd25759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "50/50 [==============================] - 115s 2s/step - loss: 0.6936 - accuracy: 0.5159 - val_loss: 0.6929 - val_accuracy: 0.4974\n",
      "Epoch 2/12\n",
      "50/50 [==============================] - 124s 2s/step - loss: 0.6964 - accuracy: 0.5057 - val_loss: 0.6929 - val_accuracy: 0.5052\n",
      "Epoch 3/12\n",
      "50/50 [==============================] - 114s 2s/step - loss: 0.6937 - accuracy: 0.5153 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 4/12\n",
      "50/50 [==============================] - 112s 2s/step - loss: 0.6937 - accuracy: 0.5178 - val_loss: 0.6947 - val_accuracy: 0.5052\n",
      "Epoch 5/12\n",
      "50/50 [==============================] - 115s 2s/step - loss: 0.6794 - accuracy: 0.5726 - val_loss: 0.7043 - val_accuracy: 0.5703\n",
      "Epoch 6/12\n",
      "50/50 [==============================] - 118s 2s/step - loss: 0.6683 - accuracy: 0.6051 - val_loss: 0.6845 - val_accuracy: 0.5156\n",
      "Epoch 7/12\n",
      "50/50 [==============================] - 119s 2s/step - loss: 0.6477 - accuracy: 0.6325 - val_loss: 0.6462 - val_accuracy: 0.6328\n",
      "Epoch 8/12\n",
      "50/50 [==============================] - 121s 2s/step - loss: 0.6429 - accuracy: 0.6312 - val_loss: 0.6572 - val_accuracy: 0.6328\n",
      "Epoch 9/12\n",
      "50/50 [==============================] - 122s 2s/step - loss: 0.6157 - accuracy: 0.6726 - val_loss: 0.7054 - val_accuracy: 0.6094\n",
      "Epoch 10/12\n",
      "50/50 [==============================] - 123s 2s/step - loss: 0.6051 - accuracy: 0.6694 - val_loss: 0.6922 - val_accuracy: 0.6224\n",
      "Epoch 11/12\n",
      "50/50 [==============================] - 125s 2s/step - loss: 0.5889 - accuracy: 0.6917 - val_loss: 0.7027 - val_accuracy: 0.6094\n",
      "Epoch 12/12\n",
      "50/50 [==============================] - 124s 2s/step - loss: 0.5840 - accuracy: 0.6847 - val_loss: 0.7086 - val_accuracy: 0.6042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197d7912cd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1602//batch_size,\n",
    "    epochs=12,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=400//batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b3193-5733-4e5e-86d7-66afa6eddf47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
